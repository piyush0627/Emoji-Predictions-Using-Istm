text,emoji
I am very happy,ğŸ˜„
I am sad,ğŸ˜¢
I love pizza,ğŸ•
This is so funny,ğŸ˜‚
It's raining outside,ğŸŒ§ï¸
I am angry,ğŸ˜ 
I love you,â¤ï¸
Feeling sleepy,ğŸ˜´
That was scary,ğŸ˜±
Good job,ğŸ‘
# Upload CSV file in Colab
from google.colab import files
uploaded = files.upload()
import pandas as pd

df = pd.read_csv("emoji_data.csv")
df.head()
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Encode labels (emojis)
le = LabelEncoder()
df['emoji_encoded'] = le.fit_transform(df['emoji'])

# Tokenize text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['text'])
X = tokenizer.texts_to_sequences(df['text'])
X = pad_sequences(X, padding='post')

# Target
y = df['emoji_encoded']
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

vocab_size = len(tokenizer.word_index) + 1
max_len = X.shape[1]
num_classes = len(le.classes_)

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_len),
    LSTM(64),
    Dense(32, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
history = model.fit(X, y, epochs=50, verbose=1)
def predict_emoji(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_len, padding='post')
    pred = model.predict(padded)
    emoji = le.inverse_transform([pred.argmax()])[0]
    return emoji

# Test it
print(predict_emoji("I am so happy"))       # ğŸ˜„
print(predict_emoji("I feel sleepy"))       # ğŸ˜´
print(predict_emoji("That was hilarious"))  # ğŸ˜‚
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
